{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem Statement:\n",
    "Women Health care requirements seem to increase at a rapid pace of 6% y-o-y and understanding these patterns could greatly influence and help in delivering expert advise and quality care \n",
    "\n",
    "A US non profit organization has provided survey related information for 3 consecutive years of various women along with 14 possible health care requirements that they needed in the last one year\n",
    "\n",
    "\n",
    "People across the United States were asked a series of over 1700 questions about their demographics, pregnancies, family planning, use of healthcare services, and medical insurance. We're focusing on the respondents to these questions that are women, and each row in the provided data represents an individual.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vkatepally\\AppData\\Local\\Continuum\\anaconda3\\envs\\py36\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2785: DtypeWarning: Columns (329,331,333,336,338,344,345,346,348,354,355,356,357,358,361,362,364,367,372,377,380,383,385,387,390,392,399,400,406,408,409,413,416,418,419,431,433,437,438,442,448,449,450,453,457,464,473,478,479,481,483,485,486,489,492,495,496,497,498,499,500,503,507,508,510,511,514,515,517,519,520,521,522,523,524,526,527,530,534,537,538,539,541,544,547,548,549,550,551,557,558,560,564,569,572,573,576,577,579,583,596,597,598,600,601,602,604,605,606,608,609,613,619,620,625,627,628,629,631,632,633,636,641,642,643,645,646,647,648,651,655,661,662,665,668,675,676,679,682,685,687,689,690,691,694,698,701,702,703,706,711,712,713,719,720,721,733,735,737,738,742,746,747,748,749,752,754,755,760,764,768,770,781,782,789,797,807,812,814,817,818,822,823,824,825,832,840,843,844,845,850,853,857,858,861,867,868,873,874,876,877,879,880,881,883,886,890,893,897,899,900,901,902,904,905,908,909,910,912,913,914,915,916,922,923,931,933,935,937,939,942,943,946,951,955,960,964,965,968,969,970,973,974,977,980,987,994,995,996,999,1000,1008,1014,1015,1016,1017,1020,1021,1023,1028,1031,1035,1036,1037,1039,1040,1043,1048,1051,1055,1058,1059,1072,1073,1074,1081,1090,1097,1098,1103,1104,1109,1112,1113,1114,1118,1120,1130,1134,1135,1139,1140,1147,1148,1149,1152,1154,1157,1158,1162,1163,1164,1166,1169,1174,1177,1180,1181,1182,1183,1185,1188,1189,1195,1197,1198,1200,1203,1208,1210,1212,1215,1217,1220,1222,1225,1229,1230,1233,1234,1241,1243,1246,1250,1251,1252,1254,1259,1262,1263,1265,1266,1269,1270,1273,1274,1276,1277,1279,1280,1282,1284,1285,1286,1289,1291,1292,1293,1294,1295,1301,1302,1304,1305,1306,1308,1309,1311,1313,1316,1318,1320,1322,1323,1325,1330,1335,1337,1340,1341,1343,1345,1350,1351,1352,1354,1357,1358,1359,1360,1361,1368,1369,1372,1377) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "df1_traindata=pd.read_csv(r'D:\\Capstone_Project\\train_Data.csv')\n",
    "df1_trainlabels=pd.read_csv(r'D:\\Capstone_Project\\train_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14644, 1379)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#shape of teh data frame\n",
    "df1_traindata.shape\n",
    "#checking if there are  any missing values\n",
    "#(df1_traindata.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14644, 1379)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#finding the duplicates and dropping if any found\n",
    "df1_traindata=df1_traindata.drop_duplicates(subset=['id','release'],keep='first')\n",
    "\n",
    "df1_traindata.shape\n",
    "#no duplcates found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14644, 259)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#our data is very sparse matrix lets take the columns which has more information \n",
    "#Consider the columns which has more than 70 percent of the data\n",
    "threshold=14644*.7\n",
    "\n",
    "columns_To_Be_Removed=[]\n",
    "\n",
    "a1=df1_traindata.isnull().sum()\n",
    "#a1\n",
    "for columnName,noOfNulls in a1.items():\n",
    "    if(noOfNulls>threshold):\n",
    "        columns_To_Be_Removed.append(columnName)\n",
    "\n",
    "    \n",
    "    \n",
    "#print(len(columns_To_Be_Removed))\n",
    "\n",
    "#dropping the columns which doen't have 70 percent of data\n",
    "\n",
    "df1_traindata_updated=df1_traindata.drop(labels=columns_To_Be_Removed,axis=1)\n",
    "df1_traindata_updated.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numeric columns 23\n",
      "categoric columns 209\n",
      "ordinal columns 25\n"
     ]
    }
   ],
   "source": [
    "#Now we have 259 columns which have more than 70 percent of data\n",
    "#finding which type of variable exist in updated data frame\n",
    "\n",
    "#Imputting the numeric columns with mean\n",
    "#Imputting the categorical and ordinal with mode\n",
    "\n",
    "no_Of_numeric_columns=0\n",
    "\n",
    "no_Of_categoric_columns=0\n",
    "\n",
    "no_Of_ordinal_columns=0\n",
    "\n",
    "for i in df1_traindata_updated.columns:\n",
    "    if(i[0]=='n'):\n",
    "        \n",
    "        imp=Imputer(missing_values='NaN',strategy='mean')\n",
    "        df1_traindata_updated[i]=imp.fit_transform(df1_traindata_updated[i].values.reshape(-1,1))\n",
    "        no_Of_numeric_columns=no_Of_numeric_columns+1\n",
    "        \n",
    "    elif(i[0]=='c'):\n",
    "        \n",
    "        #Imputer function doesn't work with the categorical values\n",
    "        \n",
    "        df1_traindata_updated[i]=df1_traindata_updated[i].fillna(df1_traindata_updated[i].value_counts().index[0])\n",
    "        #imp=Imputer(missing_values='NaN',strategy='most_frequent')\n",
    "        #df1_traindata_updated[i]=imp.fit_transform(df1_traindata_updated[i].values.reshape(-1,1))\n",
    "        no_Of_categoric_columns=no_Of_categoric_columns+1\n",
    "    elif(i[0]=='o'):\n",
    "        \n",
    "        imp=Imputer(missing_values='NaN',strategy='mean')\n",
    "        df1_traindata_updated[i]=imp.fit_transform(df1_traindata_updated[i].values.reshape(-1,1))\n",
    "        no_Of_ordinal_columns=no_Of_ordinal_columns+1\n",
    "\n",
    "\n",
    "print(\"numeric columns\",(no_Of_numeric_columns))\n",
    "print(\"categoric columns\",no_Of_categoric_columns)\n",
    "print(\"ordinal columns\",no_Of_ordinal_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id         0\n",
       "release    0\n",
       "n_0002     0\n",
       "n_0005     0\n",
       "n_0012     0\n",
       "n_0019     0\n",
       "n_0034     0\n",
       "n_0038     0\n",
       "n_0047     0\n",
       "n_0050     0\n",
       "n_0052     0\n",
       "n_0061     0\n",
       "n_0064     0\n",
       "n_0067     0\n",
       "n_0075     0\n",
       "n_0078     0\n",
       "n_0083     0\n",
       "n_0086     0\n",
       "n_0091     0\n",
       "n_0099     0\n",
       "n_0100     0\n",
       "n_0102     0\n",
       "n_0108     0\n",
       "n_0109     0\n",
       "n_0110     0\n",
       "o_0120     0\n",
       "o_0125     0\n",
       "o_0132     0\n",
       "o_0141     0\n",
       "o_0144     0\n",
       "          ..\n",
       "c_1225     0\n",
       "c_1227     0\n",
       "c_1234     0\n",
       "c_1235     0\n",
       "c_1236     0\n",
       "c_1237     0\n",
       "c_1238     0\n",
       "c_1239     0\n",
       "c_1244     0\n",
       "c_1247     0\n",
       "c_1252     0\n",
       "c_1259     0\n",
       "c_1267     0\n",
       "c_1270     0\n",
       "c_1282     0\n",
       "c_1286     0\n",
       "c_1316     0\n",
       "c_1320     0\n",
       "c_1326     0\n",
       "c_1328     0\n",
       "c_1330     0\n",
       "c_1333     0\n",
       "c_1335     0\n",
       "c_1343     0\n",
       "c_1347     0\n",
       "c_1348     0\n",
       "c_1361     0\n",
       "c_1363     0\n",
       "c_1372     0\n",
       "c_1374     0\n",
       "Length: 259, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#checking if all the columns has all the values\n",
    "df1_traindata_updated.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numeric columns 23\n",
      "categoric columns 1612\n",
      "ordinal columns 25\n",
      "Index(['id', 'n_0002', 'n_0005', 'n_0012', 'n_0019', 'n_0034', 'n_0038',\n",
      "       'n_0047', 'n_0050', 'n_0052',\n",
      "       ...\n",
      "       'c_1374_o', 'c_1374_p', 'c_1374_q', 'c_1374_r', 'c_1374_s', 'c_1374_t',\n",
      "       'c_1374_u', 'c_1374_v', 'c_1374_w', 'c_1374_x'],\n",
      "      dtype='object', length=1664)\n"
     ]
    }
   ],
   "source": [
    "#Binary class classification problem\n",
    "\n",
    "#lets consider first 1 columns of train labels\n",
    "\n",
    "a=df1_trainlabels.iloc[:,1:2]\n",
    "\n",
    "#one hot encoding\n",
    "\n",
    "d1=pd.get_dummies(df1_traindata_updated)\n",
    "d1.shape\n",
    "\n",
    "no_Of_numeric_columns=0\n",
    "\n",
    "no_Of_categoric_columns=0\n",
    "\n",
    "no_Of_ordinal_columns=0\n",
    "\n",
    "for i in d1.columns:\n",
    "    if(i[0]=='n'):\n",
    "        \n",
    "        #imp=Imputer(missing_values='NaN',strategy='mean')\n",
    "        #df1_traindata_updated[i]=imp.fit_transform(df1_traindata_updated[i].values.reshape(-1,1))\n",
    "        no_Of_numeric_columns=no_Of_numeric_columns+1\n",
    "        \n",
    "    elif(i[0]=='c'):\n",
    "        \n",
    "        #Imputer function doesn't work with the categorical values\n",
    "        \n",
    "        #df1_traindata_updated[i]=df1_traindata_updated[i].fillna(df1_traindata_updated[i].value_counts().index[0])\n",
    "        #imp=Imputer(missing_values='NaN',strategy='most_frequent')\n",
    "        #df1_traindata_updated[i]=imp.fit_transform(df1_traindata_updated[i].values.reshape(-1,1))\n",
    "        no_Of_categoric_columns=no_Of_categoric_columns+1\n",
    "        \n",
    "    elif(i[0]=='o'):\n",
    "        \n",
    "        #imp=Imputer(missing_values='NaN',strategy='mean')\n",
    "        #df1_traindata_updated[i]=imp.fit_transform(df1_traindata_updated[i].values.reshape(-1,1))\n",
    "        no_Of_ordinal_columns=no_Of_ordinal_columns+1\n",
    "\n",
    "\n",
    "print(\"numeric columns\",(no_Of_numeric_columns))\n",
    "print(\"categoric columns\",no_Of_categoric_columns)\n",
    "print(\"ordinal columns\",no_Of_ordinal_columns)\n",
    "print(d1.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "c=PCA()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
